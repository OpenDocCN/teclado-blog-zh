<html>
<head>
<title>How and when to use processes and threads in Python code</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>如何以及何时在Python代码中使用进程和线程</h1>
<blockquote>原文：<a href="https://blog.teclado.com/process-or-thread-that-is-the-question/">https://blog.teclado.com/process-or-thread-that-is-the-question/</a></blockquote><div><div class="post-content">
<p>如果你现在买一台笔记本电脑，它通常是一台四核(或更好的)电脑，有时带有T2超线程技术和T4涡轮加速技术。然而，本文不是关于这些花哨的术语。对软件工程师来说，重要的是知道如何利用不断增长的计算能力。</p><blockquote>你买了一辆法拉利，开起来像菲亚特一样。<br/><br/>——兹拉坦·伊布拉希莫维奇在巴塞罗那的日子</blockquote><h1 id="the-question">这个问题</h1><p>如果你学过操作系统相关的课程，你可能知道什么是进程和线程。有大量的教程和文章定义和比较这些概念。</p><blockquote>简单来说，进程就是一个正在执行的程序。一个或多个线程在进程的上下文中运行。线程是操作系统分配处理器时间的基本单元。一个线程可以执行进程代码的任何部分，包括另一个线程当前正在执行的部分。<br/><br/>-<a href="https://docs.microsoft.com/en-us/windows/win32/procthread/processes-and-threads" rel="nofollow">https://docs . Microsoft . com/en-us/windows/win32/proc thread/processes-and-threads</a></blockquote><p>然而，我个人发现在实践中很难联系起来:我知道我的笔记本电脑有几个内核，我知道程序可能运行在多个进程和/或多个线程上，但是它们如何协同工作，我应该选择哪一个呢？</p><h1 id="process-vs-thread">进程与线程</h1><p>在我们回答这个问题之前，让我们从编程的角度来看一下进程和线程。从天真的角度来看，你的程序<strong>是</strong>一个进程，它将在<strong>的一个</strong>内核上运行。它包含<strong>一个</strong>线程，按照你给的指令。该线程是流程的子集，因为流程包含更多信息(上下文)。</p><p>然而，随着CPU变得越来越强大，单线程进程将无法充分利用处理器。如果我们可以将工作分配到几个线程中，CPU将有足够的能力在它们之间切换，并在很短的时间内依次执行每个线程。那么看起来他们是同时在跑。这就是所谓的<strong>并发</strong>。</p><p>你可能已经猜到了，既然我们的CPU现在是多核的，为什么不让程序在所有的核上运行呢？这就是多处理发挥作用的时候了。一个进程只能在一个内核上运行。这是因为进程比线程重得多，所以CPU不能像线程那样在一个内核上无缝切换。因此，我们将它们分配到不同的核心。每个内核都有自己的计算能力，因此CPU可以同时执行与内核数量一样多的进程。这就是所谓的<strong>平行度</strong>。</p><p>现在我们知道了进程和线程在现实生活中是如何工作的，剩下的问题是:我如何选择使用哪一个？</p><h1 id="a-pythonic-answer">精辟的回答</h1><p>为了回答这个问题，我将提供两个现实生活中的例子，它们是我遇到的，并分别用多重处理和多线程解决的。希望这些例子能阐明一个概括的答案。</p><h2 id="batch-api-calls-with-multithreading">多线程批处理API调用</h2><p>一个场景是，我需要向第三方API发送一堆API调用。更具体地说，我想为我的网站获取50个用户的数据。以下是我的要求:</p><ul><li>我不关心请求的顺序，这对于并行编程来说是完美的(无论是使用并发还是并行)。</li><li>我希望它尽快完成，否则，我会让我的用户等待。</li></ul><p>我选择使用多线程而不是多重处理，因为:</p><ul><li>我没有50核的电脑。</li><li>每个任务都很轻。</li><li>我这边真的不需要任何计算。(查询数据由API主机完成，我只是请求结果。)</li></ul><p>我是这样做的(没有什么比代码更好的解释了):</p><pre><code class="language-py">from concurrent.futures import ThreadPoolExecutor, as_completed


def fetch_user_info(user_id):
    # call third party API here
    pass


with ThreadPoolExecutor(max_workers=20) as executor:
    # submit each job and map future to user id
    future_to_id = {
        executor.submit(fetch_user_info, user_id): user_id
        for user_id in user_ids
    }
    # collect each finished job
    for future in as_completed(future_to_id):
        res = future.result()  # read the future object for result
        user_id = future_to_id[future]  # match result back to user
        # TODO: process the result
</code></pre>
<p>原谅我的半伪代码，这里有一些解释。</p><ul><li><code>ThreadPoolExecutor</code>提供了一个上下文管理器，它让你不用担心线程在应该开始的时候分叉，在结束后关闭，这是一个非常好的特性。</li><li>顾名思义，<code>ThreadPoolExecutor</code>提供了一个线程“池”,您可以通过<code>max_workers</code>参数指定最多可以同时运行多少个线程。你可以将超过<code>max_workers</code>个工作分派到池中，但是一次只能接受<code>max_workers</code>个。请注意，这取决于您想要使用多少资源，而且，对我来说更重要的是，这取决于第三方API允许您发送请求的频率(我尝试了50次，但被禁止了)。</li><li><code>executor</code>返回一个Future对象，一旦任务完成，它就通过<code>concurrent.futures.as_completed</code>函数进行通信。它允许我们以同步的方式与异步代码进行交互。</li></ul><h2 id="text-processing-with-multiprocessing">多重处理的文本处理</h2><p>在这种情况下，我试图通过自然语言处理库在本地执行一些自然语言处理(NLP ):</p><ul><li>我得到了一个文章列表，NLP图书馆将从这些文章中分析和提取一些特定的术语。</li><li>就计算能力而言，NLP工作非常昂贵。这将占用大量CPU资源。</li></ul><p>我是这样做的:</p><pre><code class="language-py">from concurrent.futures import ProcessPoolExecutor, as_completed


def run_nlp(filename):
    # execute NLP
    pass


with ProcessPoolExecutor(max_workers=MAX_WORKERS) as executor:
    future_to_file = {
        executor.submit(run_nlp, filename): filename for filename in filenames
    }

    for future in as_completed(future_to_file):
        res = future.result()  # read the future object for result
        filename = future_to_file[future]  # match result back to filename
        # TODO: process the result
</code></pre>
<p>代码注释:</p><ul><li>我没有提供res解析代码，但是可以像前面的例子一样进行处理。</li><li>除了使用不同的“池执行器”之外，它几乎与前面的示例相同。对于这个例子，我只是希望提供一个不同的场景，而不是实现。</li><li>我通过一个环境变量设置了<code>max_workers</code>，我将在下面解释。</li></ul><h3 id="exploiting-the-cpu">利用CPU</h3><p>我通过环境变量配置<code>max_workers</code>的原因是，在这种情况下，并行性的水平不取决于我的用例，而是取决于计算能力。我有三个不同的环境:</p><ul><li>本地:在我的笔记本电脑上</li><li>开发:在开发服务器上</li><li>生产:在生产服务器上</li></ul><p>我的笔记本电脑是4核的，有睿频加速功能。我最初在本地将<code>MAX_WORKERS</code>设置为4，这是有意义的，因为我可以一次将4个繁重的任务分别分配给4个内核。我的开发服务器也有4个内核，生产服务器有16个内核。所以我分别设置了<code>MAX_WORKERS</code> 4，4，16。</p><p>我发现程序在本地运行比在开发服务器上运行快得多。但是当然，这两种设置都是由生产服务器提供的。通过一点研究，我发现dev服务器使用了一个更便宜的CPU，时钟频率更低。</p><p>然后我尝试了其他的东西，这导致了一个有趣的发现。我尝试局部增加<code>MAX_WORKERS</code>，结果确实有所改善。改进停止在<code>MAX_WORKERS</code> =8，这比<code>MAX_WORKERS</code> =4花费了大约一半的时间。然后，即使增加了更多的工人，这种改善也消失了。</p><p>然而，这种模式在dev服务器上并不存在。这有点奇怪，不是吗？然后，我突然意识到这一定与涡轮增压有关。它允许每个内核超出其正常速率(在短时间内大约是正常速率的两倍)，看起来好像是内核的两倍。然而，通常使用更便宜的CPU的服务器没有这种能力。</p><p>因此，我从这个例子中得出的结论是，<code>MAX_WORKERS</code>可以设置为计算机中的内核数量，以充分利用其功能。根据任务的重量和内核的能力，您甚至可以将<code>MAX_WORKERS</code>设置为内核数量的倍数。最佳配置需要一些基准测试工作。</p><h1 id="conclusion">结论</h1><p>通过这两个例子，我们可以得出这个简单的经验法则:</p><ul><li>如果您想批量启动轻量级作业，多线程是您的好朋友。</li><li>如果你的任务是资源密集型的(需要大量的计算)，考虑多重处理。</li></ul><p>还有其他限制，比如您的请求是否是上下文无关的。回想一下，我们在本文前面提到过，<strong>进程</strong>包含了<strong>线程</strong>的上下文。换句话说，线程共享相同的上下文。根据具体情况，这可能是有利的，也可能是麻烦，我们可以在另一篇文章中深入探讨这个问题。</p><p>最后，让我们花些时间来欣赏Python的魅力，它为这些问题提供了优雅的解决方案和模式。</p>
</div>
</div>    
</body>
</html>